<!DOCTYPE html>
<!--
Copyright 2017 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<!--
media_metrics uses Chrome trace events to calculate metrics about video
and audio playback. It is meant to be used for pages with a <video> or
<audio> element. It is used by videostack-eng@google.com team for
regression testing.

This metric currently supports the following measurement:
* time_to_video_play calculates how long after a video is requested to
  start playing before the video actually starts. If time_to_video_play
  regresses, then users will click to play videos and then have
  to wait longer before the videos start actually playing.
* time_to_audio_play is similar to time_to_video_play, but measures the
  time delay before audio starts playing.
* buffering_time calculates the difference between the actual play time of
  media vs its expected play time. Ideally the two should be the same.
  If actual play time is significantly longer than expected play time,
  it indicates that there were stalls during the play for buffering or
  some other reasons.
* dropped_frame_count reports the number of video frames that were dropped.
  Ideally this should be 0. If a large number of frames are dropped, the
  video play will not be smooth.
* pipeline_seek_time calculates how long it takes the media pipeline to process
  a seek operation, from when the seek request is received, to when the pipeline
  starts processing at the new location.
* seek_time calculates how long after a user requests a seek operation
  before the seek completes and the media starts playing at the new
  location, as perceived by the user.

Please inform crouleau@chromium.org and johnchen@chromium.org about
changes to this file.
-->

<link rel="import" href="/tracing/metrics/metric_registry.html">
<link rel="import" href="/tracing/model/helpers/chrome_model_helper.html">
<link rel="import" href="/tracing/value/histogram.html">

<script>
'use strict';

tr.exportTo('tr.metrics', function() {
  function mediaMetric(histograms, model) {
    const chromeHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    if (chromeHelper === undefined) return;

    for (const rendererHelper of Object.values(chromeHelper.rendererHelpers)) {
      // Find the threads we're interested in, and if a needed thread
      // is missing, no need to look further in this process.
      const mainThread = rendererHelper.mainThread;
      if (mainThread === undefined) continue;

      const compositorThread = rendererHelper.compositorThread;
      const audioThread =
        rendererHelper.process.findAtMostOneThreadNamed('AudioOutputDevice');
      if (compositorThread === undefined && audioThread === undefined) continue;

      const playStart = getPlayStart(mainThread);
      if (playStart === undefined) continue;

      const timeToVideoPlay = compositorThread === undefined ? undefined :
        getTimeToVideoPlay(compositorThread, playStart);
      const timeToAudioPlay = audioThread === undefined ? undefined :
        getTimeToAudioPlay(audioThread, playStart);

      if (timeToVideoPlay === undefined && timeToAudioPlay === undefined) {
        continue;
      }

      const droppedFrameCount = timeToVideoPlay === undefined ? undefined :
        getDroppedFrameCount(compositorThread);
      const seekTimes = getSeekTimes(mainThread);
      const bufferingTime = seekTimes.size !== 0 ? undefined :
        getBufferingTime(mainThread, playStart, timeToVideoPlay,
            timeToAudioPlay);

      if (timeToVideoPlay !== undefined) {
        histograms.createHistogram('time_to_video_play',
            tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
            timeToVideoPlay);
      }
      if (timeToAudioPlay !== undefined) {
        histograms.createHistogram('time_to_audio_play',
            tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
            timeToAudioPlay);
      }
      if (droppedFrameCount !== undefined) {
        histograms.createHistogram('dropped_frame_count',
            tr.b.Unit.byName.count_smallerIsBetter, droppedFrameCount);
      }
      for (const [key, value] of seekTimes.entries()) {
        // key is a numerical value that can have '.' when converted to string.
        // However, '.' causes problems in histogram names, so replace with '_'.
        const keyString = key.toString().replace('.', '_');
        histograms.createHistogram(
            'pipeline_seek_time_' + keyString,
            tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
            value.pipelineSeekTime);
        histograms.createHistogram(
            'seek_time_' + keyString,
            tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
            value.seekTime);
      }
      if (bufferingTime !== undefined) {
        histograms.createHistogram('buffering_time',
            tr.b.Unit.byName.timeDurationInMs_smallerIsBetter, bufferingTime);
      }
    }
  }

  function getPlayStart(mainThread) {
    let playStart;
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoLoad') {
        // TODO(johnchen@chromium.org): Support multiple audio/video
        // elements per page. Currently, we only support a single
        // audio or video element, so we can store the start time in
        // a simple variable.
        if (playStart !== undefined) {
          throw new Error(
              'Loading multiple audio/video elements not yet supported');
        }
        playStart = event.start;
      }
    }
    return playStart;
  }

  function getTimeToVideoPlay(compositorThread, playStart) {
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoRendererImpl::Render') {
        return event.start - playStart;
      }
    }
    return undefined;
  }

  function getTimeToAudioPlay(audioThread, playStart) {
    for (const event of audioThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'AudioRendererImpl::Render') {
        return event.start - playStart;
      }
    }
    return undefined;
  }

  function getSeekTimes(mainThread) {
    // We support multiple seeks per page, as long as they seek to different
    // target time. Thus the seek times are stored in a map instead of a simple
    // variable. The key of the map is event.args.target, which is a numerical
    // value indicating the target location of the seek, in unit of seconds.
    // For example, with a seek to 5 seconds mark, event.args.target === 5.
    // The value of the map is an object with 4 properties (the first two are
    // added during object creation, the latter two are added as the data
    // become available):
    // * target: seek target time (same as the map key)
    // * startTime: timestamp of the event marking start of seek
    // * pipelineSeekTime: amount of time taken by media pipeline to process
    //   this seek (milliseconds)
    // * seekTime: amount of seek time perceived by the user (milliseconds)
    // If any unexpected conditions occur, we stop processing by returning an
    // empty Map. TODO: Emit detailed warnings.
    const seekTimes = new Map();
    // |currentSeek| refers to the object associated with the seek that is
    // currently being processed. It is used to match seek end events against
    // seek start events.
    let currentSeek;
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoSeek') {
        if (currentSeek !== undefined) {
          // TODO warning 'Overlapping seek not supported'
          return new Map();
        }
        currentSeek = {
          target: event.args.target,
          startTime: event.start
        };
        seekTimes.set(event.args.target, currentSeek);
      } else if (event.title === 'WebMediaPlayerImpl::OnPipelineSeeked') {
        if (currentSeek === undefined) {
          // OK to have this event when there is no active seek, as this event
          // can be generated for other reasons, e.g., initial loading of media
          // generates this event with target of 0 seconds.
          continue;
        }
        if (currentSeek.target !== event.args.target) {
          // TODO warning 'WebMediaPlayerImpl::OnPipelineSeeked to unexpected
          // target'
          return new Map();
        }
        if (currentSeek.pipelineSeekTime !== undefined) {
          // TODO warning 'Multiple WebMediaPlayerImpl::OnPipelineSeeked events'
          return new Map();
        }
        currentSeek.pipelineSeekTime = event.start - currentSeek.startTime;
      } else if (event.title === 'WebMediaPlayerImpl::BufferingHaveEnough') {
        if (currentSeek === undefined) {
          // No current seek means this event is generated by non-seek related
          // events, e.g., initial loading of media.
          continue;
        }
        if (currentSeek.pipelineSeekTime === undefined) {
          // Since we haven't seen WebMediaPlayerImpl::OnPipelineSeeked event
          // event yet, this event is triggered by something else, e.g., a
          // have_nothing->have_enough cycle due to underflow from decoders.
          continue;
        }
        currentSeek.seekTime = event.start - currentSeek.startTime;
        // Finished processing current seek.
        currentSeek = undefined;
      }
    }
    if (currentSeek !== undefined) {
      // TODO warning 'Incomplete seek events'
      return new Map();
    }
    return seekTimes;
  }

  function getBufferingTime(mainThread, playStart, timeToVideoPlay,
      timeToAudioPlay) {
    let playEnd;
    let duration;
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::OnEnded') {
        // TODO(johnchen@chromium.org): Support multiple audio/video
        // elements per page. Currently, we only support a single
        // audio or video element, so we can store the end time in
        // a simple variable.
        if (playEnd !== undefined) {
          throw new Error(
              'Multiple media ended events not yet supported');
        }
        playEnd = event.start;
        duration = 1000 * event.args.duration;  // seconds to milliseconds
      }
    }
    if (playEnd === undefined) return undefined;
    let bufferingTime = playEnd - playStart - duration;
    if (timeToVideoPlay !== undefined) {
      bufferingTime -= timeToVideoPlay;
    } else {
      bufferingTime -= timeToAudioPlay;
    }
    return bufferingTime;
  }

  function getDroppedFrameCount(compositorThread) {
    let droppedFrameCount = 0;
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoFramesDropped') {
        droppedFrameCount += event.args.count;
      }
    }
    return droppedFrameCount;
  }

  tr.metrics.MetricRegistry.register(mediaMetric);

  return {
    mediaMetric,
  };
});
</script>
