<!DOCTYPE html>
<!--
Copyright 2017 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<!--
media_metrics uses Chrome trace events to calculate metrics about video
and audio playback. It is meant to be used for pages with a <video> or
<audio> element. It is used by videostack-eng@google.com team for
regression testing.

This metric currently supports the following measurement:
* time_to_video_play calculates how long after a video is requested to
  start playing before the video actually starts. If time_to_video_play
  regresses, then users will click to play videos and then have
  to wait longer before the videos start actually playing.
* time_to_audio_play is similar to time_to_video_play, but measures the
  time delay before audio starts playing.
* buffering_time calculates the difference between the actual play time of
  media vs its expected play time. Ideally the two should be the same.
  If actual play time is significantly longer than expected play time,
  it indicates that there were stalls during the play for buffering or
  some other reasons.
* dropped_frame_count reports the number of video frames that were dropped.
  Ideally this should be 0. If a large number of frames are dropped, the
  video play will not be smooth.
* pipeline_seek_time calculates how long it takes the media pipeline to process
  a seek operation, from when the seek request is received, to when the pipeline
  starts processing at the new location.
* seek_time calculates how long after a user requests a seek operation
  before the seek completes and the media starts playing at the new
  location, as perceived by the user.

Please inform crouleau@chromium.org and johnchen@chromium.org about
changes to this file.
-->

<link rel="import" href="/tracing/metrics/metric_registry.html">
<link rel="import" href="/tracing/model/helpers/chrome_model_helper.html">
<link rel="import" href="/tracing/value/histogram.html">

<script>
'use strict';

tr.exportTo('tr.metrics', function() {
  function mediaMetric(histograms, model) {
    const chromeHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    if (chromeHelper === undefined) return;

    for (const rendererHelper of Object.values(chromeHelper.rendererHelpers)) {
      // Find the threads we're interested in, and if a needed thread
      // is missing, no need to look further in this process.
      const mainThread = rendererHelper.mainThread;
      if (mainThread === undefined) continue;

      const compositorThread = rendererHelper.compositorThread;
      const audioThreads =
        rendererHelper.process.findAllThreadsNamed('AudioOutputDevice');
      if (compositorThread === undefined && audioThreads.length === 0) continue;

      // All the perf data we collect are stored in the following Map.
      // The keys of the map are IDs of each media element, and the value
      // associated with each key is an object containing all the perf data for
      // that media element.
      const data = new Map();

      getPlayStart(mainThread, data);
      if (data.size === 0) continue;

      if (compositorThread !== undefined) {
        getTimeToVideoPlay(compositorThread, data);
        getDroppedFrameCount(compositorThread, data);
      }

      if (audioThreads.length !== 0) {
        getTimeToAudioPlay(audioThreads, data);
      }

      getSeekTimes(mainThread, data);
      getBufferingTime(mainThread, data);

      for (const [id, elementData] of data) {
        if (elementData.timeToVideoPlay !== undefined) {
          addSample(histograms, 'time_to_video_play',
              tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
              elementData.timeToVideoPlay);
        }
        if (elementData.timeToAudioPlay !== undefined) {
          addSample(histograms, 'time_to_audio_play',
              tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
              elementData.timeToAudioPlay);
        }
        // elementData.droppedFrameCount is undefined if no frames are dropped,
        // thus we can't use it in the following if statement. Instead, we
        // report droppedFrameCount whenever there is video played.
        if (elementData.timeToVideoPlay !== undefined) {
          addSample(histograms, 'dropped_frame_count',
              tr.b.Unit.byName.count_smallerIsBetter,
              elementData.droppedFrameCount || 0);
        }
        if (!elementData.seekError && elementData.seekTimes !== undefined &&
            elementData.currentSeek === undefined) {
          for (const [key, value] of elementData.seekTimes.entries()) {
            // key is a numerical value that can have '.' when converted to
            // string. However, '.' causes problems in histogram names, so
            // replace with '_'.
            const keyString = key.toString().replace('.', '_');
            addSample(histograms, 'pipeline_seek_time_' + keyString,
                tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
                value.pipelineSeekTime);
            addSample(histograms, 'seek_time_' + keyString,
                tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
                value.seekTime);
          }
        }
        if (elementData.bufferingTime !== undefined) {
          addSample(histograms, 'buffering_time',
              tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
              elementData.bufferingTime);
        }
      }
    }
  }

  function getPlayStart(mainThread, data) {
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoLoad') {
        const id = event.args.id;
        if (data.has(id)) {
          throw new Error(
              'Unexpected multiple initialization of an audio/video elements');
        }
        data.set(id, { playStart: event.start });
      }
    }
  }

  function getTimeToVideoPlay(compositorThread, data) {
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoRendererImpl::Render') {
        const elementData = data.get(event.args.id);
        // Each video element can generte multiple Render events, one for each
        // frame. For calculating time to video play, we only use the first
        // Render event.
        if (elementData !== undefined &&
            elementData.timeToVideoPlay === undefined) {
          elementData.timeToVideoPlay = event.start - elementData.playStart;
        }
      }
    }
  }

  function getTimeToAudioPlay(audioThreads, data) {
    for (const audioThread of audioThreads) {
      for (const event of audioThread.sliceGroup.getDescendantEvents()) {
        if (event.title === 'AudioRendererImpl::Render') {
          const elementData = data.get(event.args.id);
          if (elementData !== undefined &&
              elementData.timeToAudioPlay === undefined) {
            elementData.timeToAudioPlay = event.start - elementData.playStart;
          }
        }
      }
    }
  }

  function getSeekTimes(mainThread, data) {
    // We support multiple seeks per element, as long as they seek to different
    // target time. Thus the seek times are stored in a map instead of a simple
    // variable. The key of the map is event.args.target, which is a numerical
    // value indicating the target location of the seek, in unit of seconds.
    // For example, with a seek to 5 seconds mark, event.args.target === 5.
    // The value of the map is an object with 4 properties (the first two are
    // added during object creation, the latter two are added as the data
    // become available):
    // * target: seek target time (same as the map key)
    // * startTime: timestamp of the event marking start of seek
    // * pipelineSeekTime: amount of time taken by media pipeline to process
    //   this seek (milliseconds)
    // * seekTime: amount of seek time perceived by the user (milliseconds)
    // If any unexpected conditions occur, we stop processing by returning an
    // empty Map. TODO: Emit detailed warnings.
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoSeek') {
        const elementData = data.get(event.args.id);
        if (elementData === undefined) continue;
        // |elementData.currentSeek| refers to the object associated with the
        // seek that is currently being processed for this media element.
        // It is used to match seek end events against seek start events.
        if (elementData.currentSeek !== undefined) {
          // TODO warning 'Overlapping seek not supported'
          elementData.seekError = true;
          continue;
        }
        elementData.currentSeek = {
          target: event.args.target,
          startTime: event.start
        };
        let seekTimes = elementData.seekTimes;
        if (seekTimes === undefined) {
          seekTimes = elementData.seekTimes = new Map();
        }
        seekTimes.set(event.args.target, elementData.currentSeek);
      } else if (event.title === 'WebMediaPlayerImpl::OnPipelineSeeked') {
        const elementData = data.get(event.args.id);
        if (elementData === undefined || elementData.seekError) continue;
        const currentSeek = elementData.currentSeek;
        if (currentSeek === undefined) {
          // OK to have this event when there is no active seek, as this event
          // can be generated for other reasons, e.g., initial loading of media
          // generates this event with target of 0 seconds.
          continue;
        }
        if (currentSeek.target !== event.args.target) {
          // TODO warning 'WebMediaPlayerImpl::OnPipelineSeeked to unexpected
          // target'
          elementData.seekError = true;
          continue;
        }
        if (currentSeek.pipelineSeekTime !== undefined) {
          // TODO warning 'Multiple WebMediaPlayerImpl::OnPipelineSeeked events'
          elementData.seekError = true;
          continue;
        }
        currentSeek.pipelineSeekTime = event.start - currentSeek.startTime;
      } else if (event.title === 'WebMediaPlayerImpl::BufferingHaveEnough') {
        const elementData = data.get(event.args.id);
        if (elementData === undefined || elementData.seekError) continue;
        const currentSeek = elementData.currentSeek;
        if (currentSeek === undefined) {
          // No current seek means this event is generated by non-seek related
          // events, e.g., initial loading of media.
          continue;
        }
        if (currentSeek.pipelineSeekTime === undefined) {
          // Since we haven't seen WebMediaPlayerImpl::OnPipelineSeeked event
          // event yet, this event is triggered by something else, e.g., a
          // have_nothing->have_enough cycle due to underflow from decoders.
          continue;
        }
        currentSeek.seekTime = event.start - currentSeek.startTime;
        // Finished processing current seek.
        elementData.currentSeek = undefined;
      }
    }
  }

  function getBufferingTime(mainThread, data) {
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::OnEnded') {
        const elementData = data.get(event.args.id);
        if (elementData === undefined || elementData.playStart === undefined ||
            elementData.seekTimes !== undefined) {
          continue;
        }
        if (elementData.bufferingTime !== undefined) {
          // Play was resumed after it ended previously.
          continue;
        }
        const playEnd = event.start;
        const duration = 1000 * event.args.duration;  // seconds to milliseconds
        const playTime = playEnd - elementData.playStart;
        if (elementData.timeToVideoPlay !== undefined) {
          elementData.bufferingTime =
            playTime - duration - elementData.timeToVideoPlay;
        } else if (elementData.timeToAudioPlay !== undefined) {
          elementData.bufferingTime =
            playTime - duration - elementData.timeToAudioPlay;
        }
      }
    }
  }

  function getDroppedFrameCount(compositorThread, data) {
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoFramesDropped') {
        const elementData = data.get(event.args.id);
        if (elementData === undefined) continue;
        if (elementData.droppedFrameCount === undefined) {
          elementData.droppedFrameCount = event.args.count;
        } else {
          elementData.droppedFrameCount += event.args.count;
        }
      }
    }
  }

  function addSample(histograms, name, unit, sample) {
    const histogram = histograms.getHistogramNamed(name);
    if (histogram === undefined) {
      histograms.createHistogram(name, unit, sample);
    } else {
      histogram.addSample(sample);
    }
  }

  tr.metrics.MetricRegistry.register(mediaMetric);

  return {
    mediaMetric,
  };
});
</script>
