<!DOCTYPE html>
<!--
Copyright 2017 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<!--
media_metric uses Chrome trace events to calculate metrics about video
and audio playback. It is meant to be used for pages with <video> and/or
<audio> elements. It is used by videostack-eng@google.com team for
regression testing.

This metric supports media playbacks happening simulteneous over multiple
pages, supports multiple media elements on a page, and supports multiple
playbacks with each element. It does not support media playback using
flash or any other technology not provided by Chrome videostack team.

Please inform crouleau@chromium.org and johnchen@chromium.org about
changes to this file.
-->

<link rel="import" href="/tracing/metrics/metric_registry.html">
<link rel="import" href="/tracing/model/helpers/chrome_model_helper.html">
<link rel="import" href="/tracing/value/histogram.html">

<script>
'use strict';

tr.exportTo('tr.metrics', function() {
  // MediaPlaybackMetric contains all metric values associated with a single
  // media playback.
  class MediaPlaybackMetric {
    constructor(playStartTime) {
      this.playStart_ = playStartTime;
    }

    // API methods for retrieving metric values. Each method returns undefined
    // if no value is available (e.g., timeToVideoPlay() returns undefined for
    // an audio-only playback).

    // Returns how long after a video is requested to start playing before
    // the video actually starts. If time_to_video_play regresses, then users
    // will click to play videos and then have to wait longer before the videos
    // start actually playing.
    timeToVideoPlay() {
      return this.timeToVideoPlay_;
    }

    // Similar to timeToVideoPlay, but measures the time delay before audio
    // starts playing.
    timeToAudioPlay() {
      return this.timeToAudioPlay_;
    }

    // Returns the difference between the actual play time of media vs its
    // expected play time. Ideally the two should be the same. If actual play
    // time is significantly longer than expected play time, it indicates that
    // there were stalls during the play for buffering or some other reasons.
    // Current limitation: Buffering time isn't calculated if seek occurred
    // durring playback, and it gives incorrect value if the playback isn't
    // from begining to end without pauses.
    bufferingTime() {
      return this.bufferingTime_;
    }

    // Reports the number of video frames that were dropped. Ideally this
    // should be 0. If a large number of frames are dropped, the video play
    // will not be smooth.
    droppedFrameCount() {
      // We should report dropped frame count as long as video was played.
      // An undefined this.droppedFrameCount_ means 0 frames were dropped,
      return (this.timeToVideoPlay_ !== undefined) ?
        (this.droppedFrameCount_ || 0) : undefined;
    }

    // Returns a Map containing seek times. The keys of the map arg numerical
    // values indicating the target location of the seek, in unit of seconds.
    // The values of the map are objects with the following public properties:
    // * pipelineSeekTime: amount of time taken by media pipeline to process
    //   this seek operation, from when the seek request is received, to when
    //   the pipeline starts processing at the new location, in milliseconds.
    // * seekTime: how long after a user requests a seek operation before the
    //   seek completes and the media starts playing at the new location, as
    //   perceived by the user, in milliseconds.
    seekTimes() {
      if (this.seekError_ || this.seekTimes_ === undefined ||
          this.currentSeek_ !== undefined) {
        return new Map();
      }
      return this.seekTimes_;
    }

    // API methods for processing data from trace events.

    processVideoRenderTime(videoRenderTime) {
      this.timeToVideoPlay_ = videoRenderTime - this.playStart_;
    }

    processAudioRenderTime(audioRenderTime) {
      this.timeToAudioPlay_ = audioRenderTime - this.playStart_;
    }

    processVideoFramesDropped(count) {
      if (this.droppedFrameCount_ === undefined) {
        this.droppedFrameCount_ = count;
      } else {
        this.droppedFrameCount_ += count;
      }
    }

    // We support multiple seeks per element, as long as they seek to different
    // target time. Thus the seek times are stored in a Map instead of a scalar
    // property. The key of the map is event.args.target, which is a numerical
    // value indicating the target location of the seek, in unit of seconds.
    // For example, with a seek to 5 seconds mark, event.args.target === 5.
    // The value of the map is an object with 4 properties (the first two are
    // added during object creation, the latter two are added as the data
    // become available):
    // * target: seek target time (same as the map key)
    // * startTime: timestamp of the event marking start of seek
    // * pipelineSeekTime: amount of time taken by media pipeline to process
    //   this seek (milliseconds)
    // * seekTime: amount of seek time perceived by the user (milliseconds)
    // If any unexpected conditions occur, we stop processing and set an error
    // flag this.seekError_. TODO: Emit detailed warnings.
    processDoSeek(target, startTime) {
      // currentSeek_ refers to the object associated with the
      // seek that is currently being processed for this media element.
      // It is used to match seek end events against seek start events.
      if (this.currentSeek_ !== undefined) {
        // TODO warning 'Overlapping seek not supported'
        this.seekError_ = true;
        return;
      }
      this.currentSeek_ = { target, startTime };
      let seekTimes = this.seekTimes_;
      if (seekTimes === undefined) {
        seekTimes = this.seekTimes_ = new Map();
      }
      seekTimes.set(target, this.currentSeek_);
    }

    processOnPipelineSeeked(target, time) {
      if (this.seekError_) return;
      const currentSeek = this.currentSeek_;
      if (currentSeek === undefined) {
        // OK to have this event when there is no active seek, as this event
        // can be generated for other reasons, e.g., initial loading of media
        // generates this event with target of 0 seconds.
        return;
      }
      if (currentSeek.target !== target) {
        // TODO warning 'WebMediaPlayerImpl::OnPipelineSeeked to unexpected
        // target'
        this.seekError_ = true;
        return;
      }
      if (currentSeek.pipelineSeekTime !== undefined) {
        // TODO warning 'Multiple WebMediaPlayerImpl::OnPipelineSeeked events'
        this.seekError_ = true;
        return;
      }
      currentSeek.pipelineSeekTime = time - currentSeek.startTime;
    }

    processBufferingHaveEnough(time) {
      if (this.seekError_) return;
      const currentSeek = this.currentSeek_;
      if (currentSeek === undefined) {
        // No current seek means this event is generated by non-seek related
        // events, e.g., initial loading of media.
        return;
      }
      if (currentSeek.pipelineSeekTime === undefined) {
        // Since we haven't seen WebMediaPlayerImpl::OnPipelineSeeked event
        // event yet, this event is triggered by something else, e.g., a
        // have_nothing->have_enough cycle due to underflow from decoders.
        return;
      }
      currentSeek.seekTime = time - currentSeek.startTime;
      // Finished processing current seek.
      this.currentSeek_ = undefined;
    }

    processOnEnded(playEndTime, duration) {
      // Can't calculate buffering time if there were any seeks.
      if (this.seekTimes_ !== undefined || this.seekError_) return;
      // Play was resumed after it ended previously.
      if (this.bufferingTime_ !== undefined) return;
      duration *= 1000;    // seconds to milliseconds
      const playTime = playEndTime - this.playStart_;
      if (this.timeToVideoPlay_ !== undefined) {
        this.bufferingTime_ = playTime - duration - this.timeToVideoPlay_;
      } else if (this.timeToAudioPlay !== undefined) {
        this.bufferingTime_ = playTime - duration - this.timeToAudioPlay_;
      }
    }
  }

  function mediaMetric(histograms, model) {
    const chromeHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    if (chromeHelper === undefined) return;

    for (const rendererHelper of Object.values(chromeHelper.rendererHelpers)) {
      // Find the threads we're interested in, and if a needed thread
      // is missing, no need to look further in this process.
      const mainThread = rendererHelper.mainThread;
      if (mainThread === undefined) continue;

      const compositorThread = rendererHelper.compositorThread;
      const audioThreads =
        rendererHelper.process.findAllThreadsNamed('AudioOutputDevice');
      if (compositorThread === undefined && audioThreads.length === 0) continue;

      // All the perf data we collect are stored in the following Map.
      // The keys of the map are IDs of each media playback, and the value
      // associated with each key is a MediaPlaybackMetric object containing
      // all the perf data for that playback.
      const playbackIdToMetricsMap = new Map();

      processPlayStarts(mainThread, playbackIdToMetricsMap);
      if (playbackIdToMetricsMap.size === 0) continue;

      if (compositorThread !== undefined) {
        processTimeToVideoPlays(compositorThread, playbackIdToMetricsMap);
        processDroppedFrameCount(compositorThread, playbackIdToMetricsMap);
      }

      if (audioThreads.length !== 0) {
        processTimeToAudioPlay(audioThreads, playbackIdToMetricsMap);
      }

      processSeekTimes(mainThread, playbackIdToMetricsMap);
      processBufferingTime(mainThread, playbackIdToMetricsMap);

      for (const [id, playbackMetric] of playbackIdToMetricsMap) {
        addMetricToHistograms(histograms, playbackMetric);
      }
    }
  }

  function processPlayStarts(mainThread, playbackIdToMetricsMap) {
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoLoad') {
        const id = event.args.id;
        if (playbackIdToMetricsMap.has(id)) {
          throw new Error(
              'Unexpected multiple initialization of an audio/video playback');
        }
        playbackIdToMetricsMap.set(id, new MediaPlaybackMetric(event.start));
      }
    }
  }

  function processTimeToVideoPlays(compositorThread, playbackIdToMetricsMap) {
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoRendererImpl::Render') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        // Each video playback can generate multiple Render events, one for each
        // frame. For calculating time to video play, we only use the first
        // Render event.
        if (playbackMetric !== undefined &&
            playbackMetric.timeToVideoPlay() === undefined) {
          playbackMetric.processVideoRenderTime(event.start);
        }
      }
    }
  }

  function processTimeToAudioPlay(audioThreads, playbackIdToMetricsMap) {
    for (const audioThread of audioThreads) {
      for (const event of audioThread.sliceGroup.getDescendantEvents()) {
        if (event.title === 'AudioRendererImpl::Render') {
          const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
          if (playbackMetric !== undefined &&
              playbackMetric.timeToAudioPlay() === undefined) {
            playbackMetric.processAudioRenderTime(event.start);
          }
        }
      }
    }
  }

  function processSeekTimes(mainThread, playbackIdToMetricsMap) {
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::DoSeek') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        if (playbackMetric === undefined) continue;
        playbackMetric.processDoSeek(event.args.target, event.start);
      } else if (event.title === 'WebMediaPlayerImpl::OnPipelineSeeked') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        if (playbackMetric === undefined) continue;
        playbackMetric.processOnPipelineSeeked(event.args.target, event.start);
      } else if (event.title === 'WebMediaPlayerImpl::BufferingHaveEnough') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        if (playbackMetric === undefined) continue;
        playbackMetric.processBufferingHaveEnough(event.start);
      }
    }
  }

  function processBufferingTime(mainThread, playbackIdToMetricsMap) {
    for (const event of mainThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'WebMediaPlayerImpl::OnEnded') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        if (playbackMetric === undefined) continue;
        playbackMetric.processOnEnded(event.start, event.args.duration);
      }
    }
  }

  function processDroppedFrameCount(compositorThread, playbackIdToMetricsMap) {
    for (const event of compositorThread.sliceGroup.getDescendantEvents()) {
      if (event.title === 'VideoFramesDropped') {
        const playbackMetric = playbackIdToMetricsMap.get(event.args.id);
        if (playbackMetric === undefined) continue;
        playbackMetric.processVideoFramesDropped(event.args.count);
      }
    }
  }

  function addMetricToHistograms(histograms, playbackMetric) {
    addSample(histograms, 'time_to_video_play',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        playbackMetric.timeToVideoPlay());
    addSample(histograms, 'time_to_audio_play',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        playbackMetric.timeToAudioPlay());
    addSample(histograms, 'dropped_frame_count',
        tr.b.Unit.byName.count_smallerIsBetter,
        playbackMetric.droppedFrameCount());
    for (const [key, value] of playbackMetric.seekTimes().entries()) {
      // key is a numerical value that can have '.' when converted to
      // string. However, '.' causes problems in histogram names, so
      // replace with '_'.
      const keyString = key.toString().replace('.', '_');
      addSample(histograms, 'pipeline_seek_time_' + keyString,
          tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
          value.pipelineSeekTime);
      addSample(histograms, 'seek_time_' + keyString,
          tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
          value.seekTime);
    }
    addSample(histograms, 'buffering_time',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        playbackMetric.bufferingTime());
  }

  function addSample(histograms, name, unit, sample) {
    if (sample === undefined) return;
    const histogram = histograms.getHistogramNamed(name);
    if (histogram === undefined) {
      histograms.createHistogram(name, unit, sample);
    } else {
      histogram.addSample(sample);
    }
  }

  tr.metrics.MetricRegistry.register(mediaMetric);

  return {
    mediaMetric,
  };
});
</script>
