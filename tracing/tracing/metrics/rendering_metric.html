<!DOCTYPE html>
<!--
Copyright 2018 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<link rel="import" href="/tracing/base/math/range.html">
<link rel="import" href="/tracing/base/math/range_utils.html">
<link rel="import" href="/tracing/base/math/statistics.html">
<link rel="import" href="/tracing/base/unit.html">
<link rel="import" href="/tracing/metrics/metric_registry.html">
<link rel="import" href="/tracing/model/helpers/chrome_model_helper.html">
<link rel="import" href="/tracing/model/user_model/animation_expectation.html">
<link rel="import" href="/tracing/model/user_model/segment.html">
<link rel="import" href="/tracing/value/diagnostics/breakdown.html">

<script>
'use strict';

/**
 * @fileoverview This file contains implementations of rendering metrics.
 *
 * cores_per_second_{thread group}_thread
 * ======================================
 * This set of metrics show how much a thread group was busy during animation.
 * More precisely, it shows the amount of CPU cores per seconds the thread group
 * consumes during animation. Note: the CPU usage is approximated from top-level
 * trace events in each thread; it does not come from the OS. So, the metric may
 * be noisy and not be very meaningful for threads that do not have a message
 * loop.
 *
 * These metrics replace old thread_{thread group}_cpu_time_per_frame metrics.
 * Per frame CPU times are problematic because, different animations may have
 * different frame rates, intentionally, and dividing CPU time by the number of
 * frames will be meaningless when we have a combination of animations with
 * different rates.
 *
 * Secondly, we (will) have metrics for measuring throughput/FPS. So, instead of
 * measuring CPU per frame which is a combination of CPU time and FPS, it is
 * less confusing to measure just CPU time.
 *
 * frame_times
 * ===========
 * The distribution of durations between consecutive display compositor's swap
 * buffer calls, or DRM page flips on ChromeOS devices, during animations.
 *
 * mean_frame_time
 * ===============
 * The arithmetic mean of frame_times.
 *
 * percentage_smooth
 * =================
 * The percentage of frame_times that are less than 17ms.
 *
 * TODO(chiniforooshan): This metric weighs all frames equally. So, e.g.
 * percentage_smooth is lower (worse) if we have 10 100ms-frames instead of 5
 * 1s-frames. I think it makes more sense to compute the percentage of
 * non-smooth time during animation.
 */
tr.exportTo('tr.metrics.rendering', function() {
  const UNKNOWN_THREAD_NAME = 'Unknown';

  const CATEGORY_THREAD_MAP = new Map();
  CATEGORY_THREAD_MAP.set('all', [/.*/]);
  CATEGORY_THREAD_MAP.set(
      'browser', [/^Browser Compositor$/, /^CrBrowserMain$/]);
  CATEGORY_THREAD_MAP.set('display_compositor', [/^VizCompositorThread$/]);
  CATEGORY_THREAD_MAP.set(
      'fast_path', [
        /^Browser Compositor$/, /^Chrome_InProcGpuThread$/, /^Compositor$/,
        /^CrBrowserMain$/, /^CrGpuMain$/, /IOThread/, /^VizCompositorThread$/]);
  CATEGORY_THREAD_MAP.set('gpu', [/^Chrome_InProcGpuThread$/, /^CrGpuMain$/]);
  CATEGORY_THREAD_MAP.set('io', [/IOThread/]);
  CATEGORY_THREAD_MAP.set('raster', [/CompositorTileWorker/]);
  CATEGORY_THREAD_MAP.set('renderer_compositor', [/^Compositor$/]);
  CATEGORY_THREAD_MAP.set('renderer_main', [/^CrRendererMain$/]);

  // Various tracing events.
  const DRM_EVENT = 'DrmEventFlipComplete';
  const DISPLAY_EVENT = 'BenchmarkInstrumentation::DisplayRenderingStats';
  const GESTURE_EVENT = 'SyntheticGestureController::running';

  function addValueToMap_(map, key, value) {
    const oldValue = map.get(key) || 0;
    map.set(key, oldValue + value);
  }

  function* getCategories_(threadName) {
    let isOther = true;
    for (const [category, regexps] of CATEGORY_THREAD_MAP) {
      for (const regexp of regexps) {
        if (regexp.test(threadName)) {
          if (category !== 'all') isOther = false;
          yield category;
          break;
        }
      }
    }
    if (isOther) yield 'other';
  }

  function addCoresPerSecondHistograms(histograms, model, animationSegments) {
    const totalDuration = tr.b.math.Statistics.sum(
        animationSegments, segment => segment.duration);

    // Compute and store CPU times per categories and thread name.
    const threadValues = new Map();
    for (const thread of model.getAllThreads()) {
      addValueToMap_(
          threadValues,
          thread.name || UNKNOWN_THREAD_NAME,
          tr.b.math.Statistics.sum(
              animationSegments,
              segment => thread.getCpuTimeForRange(segment.boundsRange)) /
              totalDuration);
    }
    const categoryValues = new Map();
    const breakdowns = new Map();
    for (const [threadName, coresPerSec] of threadValues) {
      for (const category of getCategories_(threadName)) {
        addValueToMap_(categoryValues, category, coresPerSec);
        if (!breakdowns.has(category)) {
          breakdowns.set(category, new tr.v.d.Breakdown());
        }
        // TODO(chiniforooshan): We break down the CPU usage of each category by
        // the thread name here. It will be more useful if we could add task
        // names too. On the other hand, breaking down at task level may be too
        // granular and we may end up with a ton of tiny slices that will not be
        // that useful.  Maybe we can break down by just top x tasks, or top x
        // (thread, task) pairs?
        //
        // Another possbility to investigate is to break down by initiator type
        // of the animation expectation.
        breakdowns.get(category).set(threadName, coresPerSec);
      }
    }

    // Build histograms.
    for (const [category, coresPerSec] of categoryValues) {
      histograms.createHistogram(
          `cores_per_second_${category}_thread`,
          tr.b.Unit.byName.unitlessNumber_smallerIsBetter,
          {
            value: coresPerSec,
            diagnostics: { breakdown: breakdowns.get(category) }
          },
          { description: 'CPU cores per second of threads during animation' });
    }
  }

  function addFrameTimeHistograms(histograms, model, animationSegments) {
    const modelHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    if (!modelHelper || !modelHelper.browserProcess) return;
    const browserProcess = modelHelper.browserProcess;

    // If DRM events exists, they are the source of truth. Otherwise, look for
    // display rendering stats. With viz, display rendering stats are emitted
    // from the GPU process; otherwise, they are emitted from the browser
    // process.
    let events = [];
    if (modelHelper.gpuHelper) {
      const gpuProcess = modelHelper.gpuHelper.process;
      events = [...gpuProcess.findTopmostSlicesNamed(DRM_EVENT)];
      if (events.length === 0) {
        events = [...gpuProcess.findTopmostSlicesNamed(DISPLAY_EVENT)];
      }
    }
    if (events.length === 0) {
      events = [...browserProcess.findTopmostSlicesNamed(DISPLAY_EVENT)];
    }
    if (events.length === 0) return;

    // Presentation Timestamps should be sorted.
    const timestamps = events.map(
        event => (
          event.title === DRM_EVENT ?
            tr.b.convertUnit(
                event.args.data['vblank.tv_sec'],
                tr.b.UnitScale.TIME.SEC, tr.b.UnitScale.TIME.MILLI_SEC) +
                tr.b.convertUnit(
                    event.args.data['vblank.tv_usec'],
                    tr.b.UnitScale.TIME.MICRO_SEC,
                    tr.b.UnitScale.TIME.MILLI_SEC) :
            event.start));

    // We use filterArray for the sake of a cleaner code. The time complexity
    // will be O(m + n log m), where m is |timestamps| and n is
    // |animationSegments|. Alternatively, we could directly loop through the
    // timestamps and animation segments here for a slightly better time
    // complexity of O(m + n).
    const frameTimes = [];
    for (const segment of animationSegments) {
      const animationTimestamps = segment.boundsRange.filterArray(timestamps);
      for (let i = 1; i < animationTimestamps.length; i++) {
        frameTimes.push(animationTimestamps[i] - animationTimestamps[i - 1]);
      }
    }

    // TODO(chiniforooshan): Figure out what kind of break down makes sense
    // here. Perhaps break down by tasks in the Viz/Browser process?
    histograms.createHistogram(
        'frame_times_tbmv2',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        frameTimes,
        { binBoundaries: tr.v.HistogramBinBoundaries.createLinear(0, 50, 20),
          description: 'Raw frame times.' });
    histograms.createHistogram(
        'mean_frame_time_tbmv2',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        tr.b.math.Statistics.mean(frameTimes),
        { binBoundaries: tr.v.HistogramBinBoundaries.createLinear(0, 50, 20),
          description: 'Arithmetic mean of frame times.' });
    histograms.createHistogram(
        'percentage_smooth_tbmv2',
        tr.b.Unit.byName.normalizedPercentage_biggerIsBetter,
        tr.b.math.Statistics.sum(frameTimes, (x => (x < 17 ? 1 : 0))) /
            frameTimes.length,
        { description: 'Percentage of frames that were hitting 60 FPS.' });
  }

  function renderingMetric(histograms, model) {
    // Find animation ranges. We use trace events produced by Telemetry to
    // detect when an animation was triggered. One drawback of this method is
    // that we cannot compute the metric from Chrome traces that are not
    // produced by Telemetry. Alternatively, we could use the user model to
    // detect animation segments in the following way:
    //
    // const animationSegments = model.userModel.segments.filter(
    //     segment => segment.expectations.find(
    //         ue => ue instanceof tr.model.um.AnimationExpectation));
    //
    // However, the user model cannot detect all types of animations, yet. For
    // more descussion about using test generated interaction records vs the
    // user mode please refer to http://bit.ly/ir-tbmv2.
    //
    // TODO(chiniforooshan): Improve the user model detection of animations.
    //
    // TODO(chiniforooshan): Move this code to an auditor (Telemetry auditor?)
    // so that other metrics can use it, too.
    const animationSegments = [];
    const IRExp = /Interaction\.([^/]+)(\/[^/]*)?$/;
    for (const thread of model.getAllThreads()) {
      for (const slice of thread.asyncSliceGroup.slices) {
        if (slice.title === GESTURE_EVENT) {
          animationSegments.push(new tr.model.um.Segment(
              slice.start, slice.duration));
        } else {
          const parts = IRExp.exec(slice.title);
          if (parts && !parts[1].startsWith('Gesture_')) {
            animationSegments.push(new tr.model.um.Segment(
                slice.start, slice.duration));
          }
        }
      }
    }
    if (animationSegments.length === 0) return;

    addCoresPerSecondHistograms(histograms, model, animationSegments);
    addFrameTimeHistograms(histograms, model, animationSegments);
  }

  tr.metrics.MetricRegistry.register(renderingMetric);

  return {
    renderingMetric,
  };
});
</script>
