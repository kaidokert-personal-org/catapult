<!DOCTYPE html>
<!--
Copyright 2018 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<link rel="import" href="/tracing/base/math/range.html">
<link rel="import" href="/tracing/base/math/range_utils.html">
<link rel="import" href="/tracing/base/math/statistics.html">
<link rel="import" href="/tracing/base/unit.html">
<link rel="import" href="/tracing/metrics/metric_registry.html">
<link rel="import" href="/tracing/model/helpers/chrome_model_helper.html">
<link rel="import" href="/tracing/model/user_model/segment.html">
<link rel="import" href="/tracing/value/diagnostics/breakdown.html">

<script>
'use strict';

/**
 * @fileoverview This file contains implementations of rendering metrics.
 *
 * cores_per_second_{thread group}_thread
 * ======================================
 * This set of metrics show how much a thread group was busy during a specific
 * segment marked by a test.  More precisely, it shows the amount of CPU cores
 * per seconds the thread group consumes during the segment. Note: the CPU usage
 * is approximated from top-level trace events in each thread; it does not come
 * from the OS. So, the metric may be noisy and not be very meaningful for
 * threads that do not have a message loop.
 *
 * frame_times
 * ===========
 * The distribution of durations between consecutive display compositor's swap
 * buffer calls, or DRM page flips on ChromeOS devices, during animations.
 *
 * mean_frame_time
 * ===============
 * The arithmetic mean of frame_times.
 *
 * percentage_smooth
 * =================
 * The percentage of frame_times that are less than 17ms.
 *
 * TODO(chiniforooshan): This metric weighs all frames equally. So, e.g.
 * percentage_smooth is lower (worse) if we have 10 100ms-frames instead of 5
 * 1s-frames. I think it makes more sense to compute the percentage of
 * non-smooth time during animation.
 */
tr.exportTo('tr.metrics.rendering', function() {
  const UNKNOWN_THREAD_NAME = 'Unknown';

  const CATEGORY_THREAD_MAP = new Map();
  CATEGORY_THREAD_MAP.set('all', [/.*/]);
  CATEGORY_THREAD_MAP.set(
      'browser', [/^Browser Compositor$/, /^CrBrowserMain$/]);
  CATEGORY_THREAD_MAP.set('display_compositor', [/^VizCompositorThread$/]);
  CATEGORY_THREAD_MAP.set(
      'fast_path', [
        /^Browser Compositor$/, /^Chrome_InProcGpuThread$/, /^Compositor$/,
        /^CrBrowserMain$/, /^CrGpuMain$/, /IOThread/, /^VizCompositorThread$/]);
  CATEGORY_THREAD_MAP.set('gpu', [/^Chrome_InProcGpuThread$/, /^CrGpuMain$/]);
  CATEGORY_THREAD_MAP.set('io', [/IOThread/]);
  CATEGORY_THREAD_MAP.set('raster', [/CompositorTileWorker/]);
  CATEGORY_THREAD_MAP.set('renderer_compositor', [/^Compositor$/]);
  CATEGORY_THREAD_MAP.set('renderer_main', [/^CrRendererMain$/]);

  // Various tracing events.
  const DRM_EVENT = 'DrmEventFlipComplete';
  const DISPLAY_EVENT = 'BenchmarkInstrumentation::DisplayRenderingStats';
  const GESTURE_EVENT = 'SyntheticGestureController::running';

  function addValueToMap_(map, key, value) {
    const oldValue = map.get(key) || 0;
    map.set(key, oldValue + value);
  }

  function* getCategories_(threadName) {
    let isOther = true;
    for (const [category, regexps] of CATEGORY_THREAD_MAP) {
      for (const regexp of regexps) {
        if (regexp.test(threadName)) {
          if (category !== 'all') isOther = false;
          yield category;
          break;
        }
      }
    }
    if (isOther) yield 'other';
  }

  function addCoresPerSecondHistograms(histograms, model, segments) {
    const totalDuration = tr.b.math.Statistics.sum(
        segments, segment => segment.duration);

    // Compute and store CPU times per categories and thread name.
    const threadValues = new Map();
    for (const thread of model.getAllThreads()) {
      addValueToMap_(
          threadValues,
          thread.name || UNKNOWN_THREAD_NAME,
          tr.b.math.Statistics.sum(
              segments,
              segment => thread.getCpuTimeForRange(segment.boundsRange)) /
              totalDuration);
    }
    const categoryValues = new Map();
    const breakdowns = new Map();
    for (const [threadName, coresPerSec] of threadValues) {
      for (const category of getCategories_(threadName)) {
        addValueToMap_(categoryValues, category, coresPerSec);
        if (!breakdowns.has(category)) {
          breakdowns.set(category, new tr.v.d.Breakdown());
        }
        // TODO(chiniforooshan): We break down the CPU usage of each category by
        // the thread name here. It will be more useful if we could add task
        // names too. On the other hand, breaking down at task level may be too
        // granular and we may end up with a ton of tiny slices that will not be
        // that useful.  Maybe we can break down by just top x tasks, or top x
        // (thread, task) pairs?
        //
        // Another possbility to investigate is to break down by initiator type
        // of the animation expectation.
        breakdowns.get(category).set(threadName, coresPerSec);
      }
    }

    // Build histograms.
    for (const [category, coresPerSec] of categoryValues) {
      histograms.createHistogram(
          `cores_per_second_${category}_thread`,
          tr.b.Unit.byName.unitlessNumber_smallerIsBetter,
          {
            value: coresPerSec,
            diagnostics: { breakdown: breakdowns.get(category) }
          },
          { description: 'CPU cores per second of a thread group' });
    }
  }

  function addFrameTimeHistograms(histograms, model, segments) {
    const modelHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    if (!modelHelper || !modelHelper.browserProcess) return;
    // If DRM events exists, they are the source of truth. Otherwise, look for
    // display rendering stats. With viz, display rendering stats are emitted
    // from the GPU process; otherwise, they are emitted from the browser
    // process.
    let events = [];
    if (modelHelper.gpuHelper) {
      const gpuProcess = modelHelper.gpuHelper.process;
      events = [...gpuProcess.findTopmostSlicesNamed(DRM_EVENT)];
      if (events.length === 0) {
        events = [...gpuProcess.findTopmostSlicesNamed(DISPLAY_EVENT)];
      }
    }
    if (events.length === 0) {
      events = [...modelHelper.browserProcess.findTopmostSlicesNamed(
          DISPLAY_EVENT)];
    }
    if (events.length === 0) return;

    // Presentation Timestamps should be sorted.
    const timestamps = events.map(
        event => (
          event.title !== DRM_EVENT ? event.start : (
            tr.b.convertUnit(
                event.args.data['vblank.tv_sec'],
                tr.b.UnitScale.TIME.SEC, tr.b.UnitScale.TIME.MILLI_SEC) +
            tr.b.convertUnit(
                event.args.data['vblank.tv_usec'],
                tr.b.UnitScale.TIME.MICRO_SEC, tr.b.UnitScale.TIME.MILLI_SEC)))
    );

    // We use filterArray for the sake of a cleaner code. The time complexity
    // will be O(m + n log m), where m is |timestamps| and n is |segments|.
    // Alternatively, we could directly loop through the timestamps and segments
    // here for a slightly better time complexity of O(m + n).
    const frameTimes = [];
    for (const segment of segments) {
      const filteredTimestamps = segment.boundsRange.filterArray(timestamps);
      for (let i = 1; i < filteredTimestamps.length; i++) {
        frameTimes.push(filteredTimestamps[i] - filteredTimestamps[i - 1]);
      }
    }

    // TODO(chiniforooshan): Figure out what kind of break down makes sense
    // here. Perhaps break down by tasks in the Viz/Browser process?
    histograms.createHistogram(
        'frame_times_tbmv2',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        frameTimes,
        { binBoundaries: tr.v.HistogramBinBoundaries.createLinear(0, 50, 20),
          description: 'Raw frame times.' });
    histograms.createHistogram(
        'mean_frame_time_tbmv2',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        tr.b.math.Statistics.mean(frameTimes),
        { binBoundaries: tr.v.HistogramBinBoundaries.createLinear(0, 50, 20),
          description: 'Arithmetic mean of frame times.' });
    histograms.createHistogram(
        'smooth_frames',
        tr.b.Unit.byName.normalizedPercentage_biggerIsBetter,
        tr.b.math.Statistics.sum(frameTimes, (x => (x < 17 ? 1 : 0))) /
            frameTimes.length,
        { description: 'Percentage of frames that were hitting 60 FPS.' });
  }

  function eventIsValidGraphicsEvent(event, eventMap) {
    if (!event.bindId || !event.args || !event.args.step) {
      return false;
    }
    const bindId = event.bindId;
    if (bindId in eventMap && event.args.step in eventMap[bindId]) {
      // It is possible for a client to submit multiple compositor frames for
      // one begin-message. So most steps can be present multiple times.
      // However, a begin-frame is issued only once, and received only once. So
      // these steps should not be repeated.
      if (event.args.step === 'IssueBeginFrame' ||
          event.args.step === 'ReceiveBeginFrame') {
        throw new Error('Unexpected duplicate step: ' + event.args.step);
      }
      return false;
    }
    return true;
  }

  function generateBreakdownForCompositorPipelineInClient(flow) {
    const breakdown = new tr.v.d.Breakdown();
    breakdown.set('time before GenerateRenderPass',
        flow.GenerateRenderPass.start - flow.ReceiveBeginFrame.start);
    breakdown.set('GenerateRenderPass duration',
        flow.GenerateRenderPass.duration);
    breakdown.set('GenerateCompositorFrame duration',
        flow.GenerateCompositorFrame.duration);
    breakdown.set('SubmitCompositorFrame duration',
        flow.SubmitCompositorFrame.duration);
    return breakdown;
  }

  function generateBreakdownForCompositorPipelineInService(flow) {
    const breakdown = new tr.v.d.Breakdown();
    breakdown.set('Processing CompositorFrame on reception',
        flow.ReceiveCompositorFrame.duration);
    breakdown.set('Delay before SurfaceAggregation',
        flow.SurfaceAggregation.start - flow.ReceiveCompositorFrame.end);
    breakdown.set('SurfaceAggregation duration',
        flow.SurfaceAggregation.duration);
    return breakdown;
  }

  function generateBreakdownForDraw(drawEvent) {
    const breakdown = new tr.v.d.Breakdown();
    for (const slice of drawEvent.subSlices) {
      breakdown.set(slice.title, slice.duration);
    }
    return breakdown;
  }

  function getDisplayCompositorThread(model) {
    const chromeHelper = model.getOrCreateHelper(
        tr.model.helpers.ChromeModelHelper);
    const gpuHelper = chromeHelper.gpuHelper;
    if (gpuHelper) {
      const thread =
          gpuHelper.process.findAtMostOneThreadNamed('VizCompositorThread');
      if (thread) {
        return thread;
      }
    }
    const browserHelper = chromeHelper.browserHelpers[0];
    return browserHelper.process.findAtMostOneThreadNamed('CrBrowserMain');
  }

  function addPipelineHistograms(histograms, model, segments) {
    let events = [];
    for (const thread of model.getAllThreads()) {
      const graphicsEvents = thread.sliceGroup.slices.filter(
          slice => {
            if (slice.title !== 'Graphics.Pipeline') return false;
            for (const segment of segments) {
              if (segment.boundsRange.containsExplicitRangeInclusive(
                  slice.start, slice.end)) {
                return true;
              }
            }
            return false;
          }
      );
      events = events.concat(graphicsEvents);
    }
    const bindEvents = {};
    for (const event of events) {
      if (!eventIsValidGraphicsEvent(event, bindEvents)) continue;
      const bindId = event.bindId;
      if (!(bindId in bindEvents)) {
        bindEvents[bindId] = {};
      }
      bindEvents[bindId][event.args.step] = event;
    }

    const dcThread = getDisplayCompositorThread(model);
    const drawEvents = {};
    if (dcThread) {
      const events =
          [...dcThread.findTopmostSlicesNamed('Graphics.Pipeline.DrawAndSwap')];
      for (const segment of segments) {
        const filteredEvents = segment.boundsRange.filterArray(events,
            evt => evt.start);
        for (const event of filteredEvents) {
          if (!event.id.startsWith(':ptr:')) continue;
          const id = parseInt(event.id.substring(5), 16);
          if (id in drawEvents) {
            throw new Error('Duplicate draw events: ' + id);
          }
          drawEvents[id] = event;
        }
      }
    }

    const issueToReceipt = histograms.createHistogram(
        'pipeline:begin_frame_transport',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        [], {
          description: 'Latency of begin-frame message from the display ' +
          'compositor to the client, including the IPC latency and task-' +
          'queue time in the client.',
        });
    const receiptToSubmit = histograms.createHistogram(
        'pipeline:begin_frame_to_frame_submission',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        [], {
          description: 'Latency between begin-frame reception and ' +
          'CompositorFrame submission in the renderer.'
        });
    const submitToAggregate = histograms.createHistogram(
        'pipeline:frame_submission_to_display',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        [], {
          description: 'Latency between CompositorFrame submission in the ' +
          'renderer to display in the display-compositor, including IPC ' +
          'latency, task-queue time in the display-compositor, and ' +
          'additional processing (e.g. surface-sync etc.)',
        });
    const aggregateToDraw = histograms.createHistogram(
        'pipeline:draw',
        tr.b.Unit.byName.timeDurationInMs_smallerIsBetter,
        [], {description: 'How long it takes for the gpu-swap step.'});

    for (const flow of Object.values(bindEvents)) {
      // Report only for the cases that go through the complete pipeline.
      if (!flow.IssueBeginFrame || !flow.ReceiveBeginFrame ||
          !flow.SubmitCompositorFrame || !flow.SurfaceAggregation) {
        continue;
      }
      issueToReceipt.addSample(flow.ReceiveBeginFrame.start -
                               flow.IssueBeginFrame.start);
      receiptToSubmit.addSample(
          flow.SubmitCompositorFrame.end - flow.ReceiveBeginFrame.start,
          {breakdown: generateBreakdownForCompositorPipelineInClient(flow)});
      submitToAggregate.addSample(
          flow.SurfaceAggregation.end - flow.SubmitCompositorFrame.end,
          {breakdown: generateBreakdownForCompositorPipelineInService(flow)});

      if (flow.SurfaceAggregation.args &&
          flow.SurfaceAggregation.args.display_trace) {
        const displayTrace = flow.SurfaceAggregation.args.display_trace;
        if (!(displayTrace in drawEvents)) continue;
        const drawEvent = drawEvents[displayTrace];
        aggregateToDraw.addSample(drawEvent.duration,
            {breakdown: generateBreakdownForDraw(drawEvent)});
      }
    }
  }

  function renderingMetric(histograms, model) {
    // Find interesting segments we want to compute metrics in. We use trace
    // events produced by Telemetry. One drawback of this method is that we
    // cannot compute the metric from Chrome traces that are not produced by
    // Telemetry. Alternatively, we could use the user model to detect
    // interesting segments, like animation segments in the following way:
    //
    // const animationSegments = model.userModel.segments.filter(
    //     segment => segment.expectations.find(
    //         ue => ue instanceof tr.model.um.AnimationExpectation));
    //
    // However, the user model cannot detect all types of animations, yet. For
    // more discussion about using test generated interaction records vs the
    // user model please refer to http://bit.ly/ir-tbmv2. TODO(chiniforooshan):
    // Improve the user model detection of animations.
    //
    // Also, some of the metrics we compute here are not animations specific.
    //
    // This code should be merged into UserModelBuilder when another metric
    // wants it, too.
    const segments = [];
    const gestureEvents = [];
    const interactionRecords = [];
    const IRExp = /Interaction\.([^/]+)(\/[^/]*)?$/;
    for (const thread of model.getAllThreads()) {
      for (const slice of thread.asyncSliceGroup.slices) {
        if (slice.title === GESTURE_EVENT) {
          gestureEvents.push(slice);
        } else if (IRExp.test(slice.title)) {
          interactionRecords.push(slice);
        }
      }
    }
    // Convert interaction record slices to segments. If an interaction record
    // contains a gesture whose time range overlaps with the interaction
    // record's range, use the gesture's time range.
    //
    // The synthetic gesture controller inserts a trace marker to precisely
    // demarcate when the gesture was running. We check for overlap, not
    // inclusion, because gesture actions can start/end slightly outside the
    // telemetry markers on Windows.
    for (const ir of interactionRecords) {
      const parts = IRExp.exec(ir.title);
      let gestureEventFound = false;
      if (parts[1].startsWith('Gesture_')) {
        for (const gestureEvent of gestureEvents) {
          if (ir.boundsRange.intersectsRangeInclusive(
              gestureEvent.boundsRange)) {
            segments.push(new tr.model.um.Segment(
                gestureEvent.start, gestureEvent.duration));
            gestureEventFound = true;
            break;
          }
        }
      }
      if (!gestureEventFound) {
        segments.push(new tr.model.um.Segment(ir.start, ir.duration));
      }
    }
    if (segments.length === 0) return;

    addCoresPerSecondHistograms(histograms, model, segments);
    addFrameTimeHistograms(histograms, model, segments);
    addPipelineHistograms(histograms, model, segments);
  }

  tr.metrics.MetricRegistry.register(renderingMetric);

  return {
    renderingMetric,
  };
});
</script>
